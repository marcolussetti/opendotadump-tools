
@misc{theopendotaprojectDataDumpMarch2017,
  title = {Data {{Dump}} ({{March}} 2011 to {{March}} 2016)},
  abstract = {About a year and a half ago, we exported our first ``data dump'' of all the parsed data we'd collected since OpenDota started operating, which consisted of over 3.5 million matches ranging from January 2015 to December 2015. After a series of adventures, we're happy to announce that we're finally...},
  language = {en},
  journal = {OpenDota},
  howpublished = {https://blog.opendota.com/2017/03/24/datadump2/},
  author = {{The OpenDota Project}},
  month = mar,
  year = {2017},
  file = {C:\\Users\\Marco\\Zotero\\storage\\EK45WHPN\\datadump2.html}
}

@misc{cuiOpenDotaAllMatches2017,
  title = {{{OpenDota}} - {{All Matches}} from {{March}} 2016 - {{Player Matches}}},
  abstract = {This is a data dump of all the parsed and unparsed Dota 2 matches from OpenDota (formerly yasp.co) as of March 2016. This is 1,191,768,403 matches. This is part 3 of three files (matches and match\_skill are the other two). This file includes all player performance data in each match. Join with matches data.},
  language = {en},
  journal = {Academic Torrents},
  howpublished = {http://academictorrents.com/details/1a0c5736bb54610ad00a45306df2b33628301409/tech},
  author = {Cui, Albert and Chung, Howard and {Hanson-Holtry}, Nicholas},
  month = mar,
  year = {2017},
  file = {C:\\Users\\Marco\\Zotero\\storage\\HWTGNQTF\\tech.html}
}

@misc{theopendotaprojectFAQ2014,
  title = {{{FAQ}}},
  abstract = {What is OpenDota? OpenDota is a volunteer-developed, open source platform providing Dota 2 data. It provides a web interface for casual users to browse through the collected data, as well as an API to allow developers to build their own applications with it. Data is collected through the Steam WebAPI,...},
  language = {en},
  howpublished = {https://blog.opendota.com/2014/08/01/faq/},
  author = {{The OpenDota Project}},
  month = aug,
  year = {2014},
  file = {C:\\Users\\Marco\\Zotero\\storage\\ILR98LDI\\faq.html}
}

@article{jinSignificanceChallengesBig2015,
  title = {Significance and {{Challenges}} of {{Big Data Research}}},
  volume = {2},
  issn = {22145796},
  doi = {10.1016/j.bdr.2015.01.006},
  language = {en},
  number = {2},
  journal = {Big Data Research},
  author = {Jin, Xiaolong and Wah, Benjamin W. and Cheng, Xueqi and Wang, Yuanzhuo},
  month = jun,
  year = {2015},
  pages = {59-64}
}

@article{urrehmanBigDataReduction2016,
  title = {Big {{Data Reduction Methods}}: {{A Survey}}},
  volume = {1},
  issn = {2364-1541},
  shorttitle = {Big {{Data Reduction Methods}}},
  doi = {10.1007/s41019-016-0022-0},
  abstract = {Research on big data analytics is entering in the new phase called fast data where multiple gigabytes of data arrive in the big data systems every second. Modern big data systems collect inherently complex data streams due to the volume, velocity, value, variety, variability, and veracity in the acquired data and consequently give rise to the 6Vs of big data. The reduced and relevant data streams are perceived to be more useful than collecting raw, redundant, inconsistent, and noisy data. Another perspective for big data reduction is that the million variables big datasets cause the curse of dimensionality which requires unbounded computational resources to uncover actionable knowledge patterns. This article presents a review of methods that are used for big data reduction. It also presents a detailed taxonomic discussion of big data reduction methods including the network theory, big data compression, dimension reduction, redundancy elimination, data mining, and machine learning methods. In addition, the open research issues pertinent to the big data reduction are also highlighted.},
  language = {en},
  number = {4},
  journal = {Data Science and Engineering},
  author = {{ur Rehman}, Muhammad Habib and Liew, Chee Sun and Abbas, Assad and Jayaraman, Prem Prakash and Wah, Teh Ying and Khan, Samee U.},
  month = dec,
  year = {2016},
  keywords = {Big data,Data complexity,Data compression,Data reduction,Dimensionality reduction},
  pages = {265-284},
  file = {C:\\Users\\Marco\\Zotero\\storage\\9PMN9U4B\\ur Rehman et al. - 2016 - Big Data Reduction Methods A Survey.pdf}
}

@article{daveyIdentificationSeasonalityTime1993,
  title = {Identification of Seasonality in Time Series: {{A}} Note},
  volume = {18},
  issn = {0895-7177},
  shorttitle = {Identification of Seasonality in Time Series},
  doi = {10.1016/0895-7177(93)90126-J},
  abstract = {Identification of patterns in time series data is critical to facilitate forecasting. One pattern that may be present is seasonality. A method is proposed which adds statistical tests of seasonal indexes to the usual autocorrelation analysis in order to identify seasonality with greater confidence. The methodology is tested with known time series. In some cases, a discrepancy arose between the original classification of the time series, which was reportedly based solely on autocorrelation analysis, and the classification which resulted from the statistical tests. Further analysis, which included examination of time series plots, indicated that these discrepancies were likely to occur in series with possible structural changes. Out-of-sample forecasts were performed with seasonal and deseasonalized data to determine the benefits of the ``corrected'' classification. Results show improvement, albeit small.},
  number = {6},
  journal = {Mathematical and Computer Modelling},
  author = {Davey, A. M. and Flores, B. E.},
  month = sep,
  year = {1993},
  pages = {73-81},
  file = {C:\\Users\\Marco\\Zotero\\storage\\GU75UJTN\\Davey and Flores - 1993 - Identification of seasonality in time series A no.pdf;C:\\Users\\Marco\\Zotero\\storage\\5V3I88HU\\089571779390126J.html}
}

@article{namiotBigDataStream2015,
  title = {On {{Big Data Stream Processing}}},
  volume = {3},
  copyright = {Copyright (c) 2015 International Journal of Open Information Technologies},
  issn = {2307-8162},
  abstract = {In this paper, we would like to discuss data stream processing in the big data area. Our goal is to provide a quick introduction and survey of the technical solutions for big data streams processing. In this survey, we target Machine to Machine communications, sensors fusion in Internet of Things as well as time series data processing. We discuss the basic elements behind data streams processing, the existing technical solutions for their implementations as well some prospect system architectures.},
  language = {en},
  number = {8},
  journal = {International Journal of Open Information Technologies},
  author = {Namiot, Dmitry},
  month = aug,
  year = {2015},
  pages = {48-51},
  file = {C:\\Users\\Marco\\Zotero\\storage\\TPKXGUIG\\Namiot - 2015 - On Big Data Stream Processing.pdf;C:\\Users\\Marco\\Zotero\\storage\\IW7UDL6V\\225.html}
}

@inproceedings{al-mansooriSurveyBigData2018,
  address = {Brisband, Queensland, Australia},
  title = {A Survey on Big Data Stream Processing in {{SDN}} Supported Cloud Environment},
  isbn = {978-1-4503-5436-3},
  doi = {10.1145/3167918.3167924},
  language = {en},
  booktitle = {Proceedings of the {{Australasian Computer Science Week Multiconference}} on   - {{ACSW}} '18},
  publisher = {{ACM Press}},
  author = {{Al-Mansoori}, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav},
  year = {2018},
  pages = {1-11}
}

@article{iyerWhatDrivesBig2013,
  title = {What Drives {{Big Data Analytics}} to {{Cloud}}},
  volume = {2},
  abstract = {PDF | The key transformational platform for data between sourcing and storage is data analytics. Analysis of data can be achieved using either traditional stand-alone computers or by moving data analytics to the cloud. Given the volume, velocity and variety of data, knowledge...},
  language = {en},
  journal = {International Journal of Consumer \& Business Analytics},
  author = {Iyer, Easwar Krishna and Sood, Sachin and Gupta, Neha and Panda, Tapan},
  month = dec,
  year = {2013},
  pages = {68-84},
  file = {C:\\Users\\Marco\\Zotero\\storage\\6XJJ7V9U\\267026975_What_drives_Big_Data_Analytics_to_Cloud.html}
}

@misc{elmirghaniCuttingBigData2018,
  title = {Cutting {{Big Data Down}} to {{Size}}},
  abstract = {IEEE Green ICT Initiative seeks to reduce the technology's impact on the environment},
  journal = {IEEE - The Institute},
  howpublished = {http://theinstitute.ieee.org/ieee-roundup/blogs/blog/cutting-big-data-down-to-size},
  author = {Elmirghani, Jaafar},
  month = jan,
  year = {2018},
  file = {C:\\Users\\Marco\\Zotero\\storage\\35RVUF4R\\cutting-big-data-down-to-size.html}
}

@misc{univocitysoftwareptyltdComparisonsAllJavabased2018,
  title = {Comparisons among All {{Java}}-Based {{CSV}} Parsers in Existence: {{uniVocity}}/Csv-Parsers-Comparison},
  copyright = {View license},
  shorttitle = {Comparisons among All {{Java}}-Based {{CSV}} Parsers in Existence},
  howpublished = {univocity},
  author = {{uniVocity Software Pty Ltd}},
  month = oct,
  year = {2018}
}

@misc{vorontsovTroveLibraryUsing2014,
  title = {Trove Library: Using Primitive Collections for Performance},
  shorttitle = {Trove Library},
  abstract = {This article describes Trove library - set of primitive value collections for Java as well as migration from JDK to Trove collections},
  language = {en-US},
  journal = {Java Performance Tuning Guide},
  author = {Vorontsov, Mikhail},
  month = jul,
  year = {2014},
  file = {C:\\Users\\Marco\\Zotero\\storage\\4287M2AV\\primitive-types-collections-trove-library.html}
}

@misc{dota2wikicontributorsCategoryPatches,
  title = {Category:{{Patches}}},
  shorttitle = {Category},
  language = {en},
  journal = {Dota 2 Wiki},
  howpublished = {https://dota2.gamepedia.com/Category:Patches},
  author = {{Dota 2 Wiki Contributors}},
  file = {C:\\Users\\Marco\\Zotero\\storage\\AAK9CNKR\\CategoryPatches.html}
}

@book{scharfStatisticalSignalProcessing1991,
  address = {Reading, Mass},
  edition = {1 edition},
  title = {Statistical {{Signal Processing}}: {{Detection}}, {{Estimation}}, and {{Time Series Analysis}}},
  isbn = {978-0-201-19038-0},
  shorttitle = {Statistical {{Signal Processing}}},
  abstract = {This book embraces the many mathematical procedures that engineers and statisticians use to draw inference from imperfect or incomplete measurements.   This book presents the fundamental ideas in statistical signal processing along four distinct lines: mathematical and statistical preliminaries; decision theory; estimation theory; and time series analysis.},
  language = {English},
  publisher = {{Pearson}},
  author = {Scharf, Louis L.},
  month = jul,
  year = {1991},
  file = {C:\\Users\\Marco\\Zotero\\storage\\8QUBTJDP\\Scharf - 1991 - Statistical Signal Processing Detection, Estimati.pdf}
}

@article{triaccaLessonAutocovarianceFunction,
  title = {Lesson 5: {{The Autocovariance Function}} of a Stochastic Process},
  language = {en},
  author = {Triacca, Umberto},
  pages = {18},
  file = {C:\\Users\\Marco\\Zotero\\storage\\RPQV5P2U\\Triacca - Lesson 5 The Autocovariance Function of a stochas.pdf}
}

@book{cryerTimeSeriesAnalysis2010,
  address = {New York},
  edition = {2nd edition},
  title = {Time {{Series Analysis}}: {{With Applications}} in {{R}}},
  isbn = {978-0-387-75958-6},
  shorttitle = {Time {{Series Analysis}}},
  abstract = {This book has been developed for a one-semester course usually attended by students in statistics, economics, business, engineering, and quantitative social sciences. A unique feature of this edition is its integration with the R computing environment. Basic applied statistics is assumed through multiple regression. Calculus is assumed only to the extent of minimizing sums of squares but a calculus-based introduction to statistics is necessary for a thorough understanding of some of the theory. Actual time series data drawn from various disciplines are used throughout the book to illustrate the methodology.},
  language = {English},
  publisher = {{Springer}},
  author = {Cryer, Jonathan D. and Chan, Kung-Sik},
  month = nov,
  year = {2010},
  file = {C:\\Users\\Marco\\Zotero\\storage\\5UNKSGHC\\Cryer and Chan - 2010 - Time Series Analysis With Applications in R.pdf}
}

@article{chenDataintensiveApplicationsChallenges2014,
  title = {Data-Intensive Applications, Challenges, Techniques and Technologies: {{A}} Survey on {{Big Data}}},
  volume = {275},
  issn = {00200255},
  shorttitle = {Data-Intensive Applications, Challenges, Techniques and Technologies},
  doi = {10.1016/j.ins.2014.01.015},
  abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as dataintensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.},
  language = {en},
  journal = {Information Sciences},
  author = {Chen, C.L. Philip and Zhang, Chun-Yang},
  month = aug,
  year = {2014},
  pages = {314-347},
  file = {C:\\Users\\Marco\\Zotero\\storage\\2Z4KWQ2C\\Philip Chen and Zhang - 2014 - Data-intensive applications, challenges, technique.pdf}
}

@inproceedings{lussettiBigDataReduction2019,
  address = {Kamloops, Canada},
  title = {Big {{Data Reduction}}: {{Lessons Learned}} from {{Analyzing One Billion Dota}} 2 {{Matches}}},
  shorttitle = {Big {{Data Reduction}}},
  booktitle = {14th Annual {{TRU Undergraduate Research}} \& {{Innovation Conference}}},
  author = {Lussetti, Marco and Fraser, Dyson},
  month = mar,
  year = {2019},
  file = {C:\\Users\\Marco\\Zotero\\storage\\C4NDFMZV\\26.html}
}

@misc{poreMustKnowWhatCurse2017,
  title = {Must-{{Know}}: {{What}} Is the Curse of Dimensionality?},
  shorttitle = {Must-{{Know}}},
  language = {en-US},
  journal = {KDnuggets},
  author = {Pore, Prasad},
  month = apr,
  year = {2017},
  file = {C:\\Users\\Marco\\Zotero\\storage\\VD68N7NH\\must-know-curse-dimensionality.html}
}

@inproceedings{yaoGranularComputing2004,
  title = {Granular {{Computing}}},
  volume = {31},
  abstract = {The basic ideas and principles of granular computing (GrC) have been studied explicitly or implicitly in many fields in isolation. With the recent renewed and fast growing interest, it is time to extract the commonality from a diversity of fields and to study systematically and formally the domain independent principles of granular computing in a unified model. A framework of granular computing can be established by applying its own principles. We examine such a framework from two perspectives, granular computing as structured thinking and structured problem solving. From the philosophical perspective or the conceptual level, granular computing focuses on structured thinking based on multiple levels of granularity. The implementation of such a philosophy in the application level deals with structured problem solving.},
  booktitle = {Proceedings of {{The}} 4th {{Chinese National Conference}} on {{Rough Sets}} and {{Soft Computing}}},
  author = {Yao, Yiyu},
  year = {2004},
  keywords = {Granular computing,Problem solving,Theory,Umbrella term,Unified Model},
  pages = {1-5},
  file = {C:\\Users\\Marco\\Zotero\\storage\\UIPX7ZLI\\grc.pdf}
}

@misc{wenJsoniterJsoniterator2018,
  title = {Jsoniter (Json-Iterator)},
  copyright = {MIT},
  shorttitle = {Jsoniter (Json-Iterator) Is Fast and Flexible {{JSON}} Parser Available in {{Java}} and {{Go}}},
  howpublished = {Jsoniter},
  author = {Wen, Tao},
  month = jul,
  year = {2018}
}

@misc{univocitysoftwareptyltdUniVocityparsers2019,
  title = {{{uniVocity}}-Parsers},
  copyright = {View license},
  howpublished = {univocity},
  author = {{uniVocity Software Pty Ltd}},
  month = feb,
  year = {2019}
}

@misc{rogerSimpleFlatMapper2019,
  title = {{{SimpleFlatMapper}}},
  copyright = {MIT},
  shorttitle = {Fast and {{Easy}} Mapping from Database and Csv to {{POJO}}. {{A}} Java Micro {{ORM}}, Lightweight Alternative to {{iBatis}} and {{Hibernate}}. {{Fast Csv Parser}} and {{Csv Mapper}}},
  author = {Roger, Arnaud},
  month = mar,
  year = {2019}
}

@misc{popmaPicocli2019,
  title = {Picocli},
  copyright = {Apache-2.0},
  author = {Popma, Remko},
  month = feb,
  year = {2019}
}

@misc{ruckerjonesOpencsv2019,
  title = {Opencsv},
  author = {Rucker Jones, Andrew and Conway, Scott and {opencsv Contributors}},
  month = feb,
  year = {2019},
  file = {C:\\Users\\Marco\\Zotero\\storage\\HG3WVXYC\\opencsv.sourceforge.net.html}
}

@misc{edenGNUTroveTrove4j2013,
  title = {{{GNU Trove}} / Trove4j},
  abstract = {The Trove library provides high speed Object and primitive collections for Java.},
  author = {Eden, Rob and Parent, Johan and Randall, Jeff and Friedman, Eric D. and {Trove Development Team}},
  month = feb,
  year = {2013},
  file = {C:\\Users\\Marco\\Zotero\\storage\\YBAYBWX5\\trove.html}
}


